{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ORZgzk52GSAT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "T9-iyV5oy94Z",
        "outputId": "bd875a04-652a-4023-aed9-0d6550ff7821",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split validation set (10% of training data)\n",
        "val_size = int(0.1 * len(x_train))\n",
        "x_val, y_val = x_train[:val_size], y_train[:val_size]\n",
        "x_train, y_train = x_train[val_size:], y_train[val_size:]\n",
        "\n",
        "# Normalize inputs\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
        "x_val = x_val.reshape(x_val.shape[0], -1) / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0"
      ],
      "metadata": {
        "id": "BeTHNHD3zOwS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0YRHjFaqy9va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M6KcTRZyy9lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9kos9zqy84O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "  def __init__(self,input_shape,output_shape,n_hidden_layers,h_per_layer,activation_func=\"relu\",loss_func=\"cross_entropy_loss\",init_type=\"random\"):\n",
        "    self.input_shape = input_shape\n",
        "    self.output_shape = output_shape\n",
        "    self.n_h = n_hidden_layers\n",
        "    self.k = h_per_layer\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "    self.grad_weights = []\n",
        "    self.grad_biases = []\n",
        "    self.activation_func = activation_func\n",
        "    self.loss_func = loss_func\n",
        "  def activation(self,x):\n",
        "    if self.activation_func == \"relu\":\n",
        "      return np.maximum(0,x)\n",
        "    elif self.activation_func == \"tanh\":\n",
        "      return np.tanh(x)\n",
        "    elif self.activation_func == \"sigmoid\":\n",
        "      return 1/(1+np.exp(-x))\n",
        "  def activation_grad(self,x):\n",
        "    if self.activation_func == \"relu\":\n",
        "      return np.where(x>0,1,0)\n",
        "    elif self.activation_func == \"tanh\":\n",
        "      return 1-np.tanh(x)**2\n",
        "    elif self.activation_func == \"sigmoid\":\n",
        "      s = 1/(1+np.exp(-x))\n",
        "      return s*(1-s)\n",
        "  def softmax(self,x):\n",
        "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
        "  def transpose(self,w):\n",
        "    return np.array(w).T.tolist()\n",
        "  def one_hot(self,y):\n",
        "    return np.eye(self.output_shape)[y]\n",
        "  def loss(self,y_hat,y_true):\n",
        "    if self.loss_func == \"cross_entropy_loss\":\n",
        "      return -np.sum(y_true*np.log(y_hat))\n",
        "    elif self.loss_func == \"squared_error\":\n",
        "      return np.sum((y_hat-y_true)**2)\n",
        "  def accuracy(self,y_hat,y_true):\n",
        "    return np.sum(y_hat==y_true)/len(y_true)\n",
        "  def weight_init(self,init_type=\"random\",n=1):\n",
        "    if init_type == \"random\":\n",
        "      self.weights.append(np.random.randn(self.input_shape,self.k))\n",
        "      self.biases.append(np.random.randn(self.k,n))\n",
        "      for i in range(self.n_h-1):\n",
        "        self.weights.append(np.random.randn(self.k,self.k))\n",
        "        self.biases.append(np.random.randn(self.k,n))\n",
        "      self.weights.append(np.random.randn(self.k,self.output_shape))\n",
        "      self.biases.append(np.random.randn(self.output_shape,n))\n",
        "    return self.weights,self.biases\n",
        "  def basic_optimizer(self,lr):\n",
        "    self.weights = self.weights - lr*self.grad_weights\n",
        "    self.biases = self.biases - lr*self.grad_biases\n",
        "  def forward(self,x,y):\n",
        "    self.weights,self.biases = self.weight_init(n=x.shape[0])\n",
        "    a_list = [0]*(self.n_h+2)\n",
        "    h_list = [0]*(self.n_h+2)\n",
        "    h = self.transpose(x)\n",
        "    h_list[0] = h\n",
        "    for i in range(self.n_h):\n",
        "      a_list[i+1] = np.dot(self.transpose(self.weights[i]),h_list[i])+self.biases[i]\n",
        "      h_list[i+1] = self.activation(a_list[i+1])\n",
        "    a_list[self.n_h+1] = np.dot(self.transpose(self.weights[self.n_h]),h_list[self.n_h])+self.biases[self.n_h]\n",
        "    y_hat = self.softmax(a_list[self.n_h+1])\n",
        "    h_list[self.n_h+1] = y_hat\n",
        "    return a_list,h_list\n",
        "  def backward(self,a_list,h_list,y):\n",
        "    a_grad_list = [0]*(self.n_h+2)\n",
        "    h_grad_list = [0]*(self.n_h+2)\n",
        "    y_hat = h_list[self.n_h+1]\n",
        "    if self.loss_func == \"cross_entropy_loss\": #gradient wrt output layer\n",
        "      a_grad = y_hat - self.one_hot(y)\n",
        "    elif self.loss_func == \"squared_error\":\n",
        "      a_grad = 2*(y_hat - y)\n",
        "    a_grad_list[-1] = a_grad\n",
        "    for k in range(self.n_h,-1,-1): # gradient wrt hiddden layers\n",
        "      h_grad = np.dot(self.transpose(self.weights[k+1]),a_grad_list[k+1])\n",
        "      h_grad_list[k] = h_grad\n",
        "      a_grad = np.multiply(h_grad,self.activation_grad(a_list[k]))\n",
        "      a_grad_list[k] = a_grad\n",
        "      self.grad_weights[k] = np.dot(a_grad_list[k+1],h_list[k].T) #gradients wrt parameters\n",
        "      self.grad_biases[k] = a_grad_list[k+1]\n",
        "  def train(self,x_train,y_train,x_val,y_val,epochs):\n",
        "    for i in range(epochs):\n",
        "      a_list_train,h_list_train = self.forward(x_train,y_train) # forward pass\n",
        "      print(\"forward pass done\")\n",
        "      self.backward(a_list_train,h_list_train,y_train) # backward pass\n",
        "      print(\"backward pass done\")\n",
        "      self.basic_optimizer(0.01) # updating weights\n",
        "      print(\"optimizer done\")\n",
        "      y_hat = h_list_train[self.n_h+1]\n",
        "      train_loss = self.loss(self.transpose(y_hat),self.one_hot(y_train))\n",
        "      train_acc = self.accuracy(np.argmax(np.array(self.transpose(y_hat)),axis=0),y_train)\n",
        "      a_list_val,h_list_val = self.forward(x_val,y_val)\n",
        "      y_hat_val = h_list_val[self.n_h+1]\n",
        "      val_loss = self.loss(y_hat_val,self.one_hot(y_val))\n",
        "      val_acc = self.accuracy(np.argmax(np.array(y_hat_val),axis=0),y_val)\n",
        "      print(\"Epoch: \",i+1,\"Train Loss: \",train_loss,\"Train Accuracy: \",train_acc,\"Val Loss: \",val_loss,\"Val Accuracy: \",val_acc)"
      ],
      "metadata": {
        "id": "5BE2JhcCJoUV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_hid_layers = 3\n",
        "hid_layer_size = 64  # Changeable number of layers\n",
        "weight_init = 'xavier'  # Options: 'random', 'xavier'\n",
        "activation = 'relu'  # Options: 'sigmoid', 'tanh', 'relu'\n",
        "#l2_reg = 0.0005  # L2 regularization strength\n",
        "learning_rate = 1e-3  # Options: 1e-3, 1e-4\n",
        "#batch_size = 32  # Options: 16, 32, 64\n",
        "epochs = 5  # Options: 5, 10\n",
        "#optimizer = 'sgd'  # Options: 'sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam'\n",
        "loss_func = 'cross_entropy_loss'  # Options: 'cross_entropy_loss', 'squared_error'\n",
        "# self,input_shape,output_shape,n_hidden_layers,h_per_layer,activation_func=\"ReLU\",loss_func=\"cross_entropy_loss\"\n",
        "# Initialize and train the model\n",
        "nn = NN(input_shape=784,output_shape=10,n_hidden_layers=num_hid_layers, h_per_layer=hid_layer_size, activation_func=activation, init_type=weight_init, loss_func=loss_func)\n",
        "nn.train(x_train, y_train, x_val, y_val, epochs=epochs)"
      ],
      "metadata": {
        "id": "lQgXsVYBy4XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "_U3OP1_Yy4Tz",
        "outputId": "a26449c1-022a-44e3-ae21-2fe38111f21c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCTWrR1By4RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S_IjmXtRy4OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5VGQ3r6Cy4Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8pNfdXxy3-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}